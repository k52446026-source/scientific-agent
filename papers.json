[
  {
    "title": "Revisiting Generalization Across Difficulty Levels: It's Not So Easy",
    "authors": [
      "Yeganeh Kordi",
      "Nihal V. Nayak",
      "Max Zuo",
      "Ilana Nguyen",
      "Stephen H. Bach"
    ],
    "published": "2025-11-26T18:59:57+00:00",
    "entry_id": "http://arxiv.org/abs/2511.21692v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21692v1",
    "summary": "We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.",
    "primary_category": "cs.CL",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
    "authors": [
      "Yusuf Dalva",
      "Guocheng Gordon Qian",
      "Maya Goldenberg",
      "Tsai-Shien Chen",
      "Kfir Aberman",
      "Sergey Tulyakov",
      "Pinar Yanardag",
      "Kuan-Chieh Jackson Wang"
    ],
    "published": "2025-11-26T18:59:56+00:00",
    "entry_id": "http://arxiv.org/abs/2511.21691v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21691v1",
    "summary": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.",
    "primary_category": "cs.CV",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos",
    "authors": [
      "Seungjae Lee",
      "Yoonkyo Jung",
      "Inkook Chun",
      "Yao-Chih Lee",
      "Zikui Cai",
      "Hongjia Huang",
      "Aayush Talreja",
      "Tan Dat Dao",
      "Yongyuan Liang",
      "Jia-Bin Huang",
      "Furong Huang"
    ],
    "published": "2025-11-26T18:59:55+00:00",
    "entry_id": "http://arxiv.org/abs/2511.21690v1",
    "pdf_url": "https://arxiv.org/pdf/2511.21690v1",
    "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.",
    "primary_category": "cs.RO",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ]
  }
]